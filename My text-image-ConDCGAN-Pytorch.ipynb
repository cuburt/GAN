{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0_TostjTJl_","executionInfo":{"status":"ok","timestamp":1726826854151,"user_tz":-480,"elapsed":109505,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}},"outputId":"b1ffdeb5-88f3-448c-dabb-aa547d33face"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kEg-7SC_ag4e","executionInfo":{"status":"ok","timestamp":1726826868761,"user_tz":-480,"elapsed":7475,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef55baca-226d-48dc-c37c-d21ed70f64e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Collecting h5py==3.6.0\n","  Downloading h5py-3.6.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.9 kB)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.6.0) (1.26.4)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Downloading h5py-3.6.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n","Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, h5py, torchmetrics, pytorch-lightning\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.11.0\n","    Uninstalling h5py-3.11.0:\n","      Successfully uninstalled h5py-3.11.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.0 requires h5py>=3.10.0, but you have h5py 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h5py-3.6.0 lightning-utilities-0.11.7 pytorch-lightning-2.4.0 torchmetrics-1.4.2\n"]}],"source":["!pip install pytorch-lightning transformers torch torchvision matplotlib opencv-python h5py==3.6.0"]},{"cell_type":"code","source":["import torch\n","\n","def process_caption(caption, max_caption_length=200, alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{} \"):\n","    \"\"\"\n","    Converts a caption string to a tensor of one-hot encoded vectors based on the given alphabet.\n","\n","    Args:\n","        caption (str): The input caption to be converted to one-hot encoded vectors.\n","        max_caption_length (int): The maximum length of the output label sequence. If the caption is longer, it will be truncated.\n","        alphabet (str, optional): The alphabet used to map characters to numeric labels and define the length of the one-hot vectors. Default is a combination of lowercase letters, numbers, and common punctuation marks.\n","\n","    Returns:\n","        torch.Tensor: A tensor containing the one-hot encoded vectors for the caption.\n","    \"\"\"\n","    # Convert the caption to lowercase for case-insensitivity\n","    caption = caption.lower()\n","\n","    # Create a mapping from characters in the alphabet to numeric labels\n","    alpha_to_num = {k: v + 1 for k, v in zip(alphabet, range(len(alphabet)))}\n","\n","    # Initialize the output tensor with zeros and set the data type to long\n","    labels = torch.zeros(max_caption_length).long()\n","\n","    # Determine the maximum number of characters to process from the caption\n","    max_i = min(max_caption_length, len(caption))\n","\n","    # Convert each character in the caption to its corresponding numeric label\n","    for i in range(max_i):\n","        # If the character is not in the alphabet, use the numeric label for space (' ')\n","        labels[i] = alpha_to_num.get(caption[i], alpha_to_num[' '])\n","\n","    labels = labels.unsqueeze(1)\n","\n","    # Convert the numeric labels to one-hot encoded vectors\n","    # Initialize a tensor of zeros with the shape (sequence length, alphabet length + 1) and scatter ones based on the labels\n","    one_hot = torch.zeros(labels.size(0), len(alphabet) + 1).scatter_(1, labels, 1.)\n","\n","    # Remove the column corresponding to the numeric label 0 (used for padding)\n","    one_hot = one_hot[:, 1:]\n","\n","    # Permute the tensor to have the sequence length as the first dimension\n","    one_hot = one_hot.permute(1, 0)\n","\n","    return one_hot\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)"],"metadata":{"id":"A3FC1Kb5R7FH","executionInfo":{"status":"ok","timestamp":1726826881401,"user_tz":-480,"elapsed":6481,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Copied from https://github.com/aelnouby/Text-to-Image-Synthesis\n","\n","import os\n","import io\n","from torch.utils.data import Dataset, DataLoader\n","import h5py\n","import numpy as np\n","import pdb\n","from PIL import Image\n","import torch\n","from torch.autograd import Variable\n","import pdb\n","import torch.nn.functional as F\n","\n","class Text2ImageDataset(Dataset):\n","\n","    def __init__(self, datasetFile, transform=None, split=0):\n","        self.datasetFile = datasetFile\n","        self.transform = transform\n","        self.dataset = None\n","        self.dataset_keys = None\n","        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n","        self.h5py2int = lambda x: int(np.array(x))\n","\n","    def __len__(self):\n","        f = h5py.File(self.datasetFile, 'r')\n","        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n","        length = len(f[self.split])\n","        f.close()\n","\n","        return length\n","\n","    def __getitem__(self, idx):\n","        if self.dataset is None:\n","            self.dataset = h5py.File(self.datasetFile, mode='r')\n","            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n","\n","        example_name = self.dataset_keys[idx]\n","        example = self.dataset[self.split][example_name]\n","\n","        # pdb.set_trace()\n","\n","        right_image = bytes(np.array(example['img']))\n","        right_embed = np.array(example['embeddings'], dtype=float)\n","        wrong_image = bytes(np.array(self.find_wrong_image(example['class'])))\n","        inter_embed = np.array(self.find_inter_embed())\n","\n","        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n","        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n","\n","        right_image = self.validate_image(right_image)\n","        wrong_image = self.validate_image(wrong_image)\n","\n","        try:\n","            txt = np.array(example['txt']).astype(str)\n","        except:\n","\n","            txt = np.array([example['txt'][()].decode('utf-8', errors='replace')])\n","            txt = np.char.replace(txt, '�', ' ').astype(str)\n","\n","        sample = {\n","                'right_images': torch.FloatTensor(right_image),\n","                'right_embed': torch.FloatTensor(right_embed),\n","                'wrong_images': torch.FloatTensor(wrong_image),\n","                'inter_embed': torch.FloatTensor(inter_embed),\n","                'txt': str(txt)\n","                 }\n","\n","        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n","        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n","\n","        return sample\n","\n","    def find_wrong_image(self, category):\n","        idx = np.random.randint(len(self.dataset_keys))\n","        example_name = self.dataset_keys[idx]\n","        example = self.dataset[self.split][example_name]\n","        _category = example['class']\n","\n","        if _category != category:\n","            return example['img']\n","\n","        return self.find_wrong_image(category)\n","\n","    def find_inter_embed(self):\n","        idx = np.random.randint(len(self.dataset_keys))\n","        example_name = self.dataset_keys[idx]\n","        example = self.dataset[self.split][example_name]\n","        return example['embeddings']\n","\n","\n","    def validate_image(self, img):\n","        img = np.array(img, dtype=float)\n","        if len(img.shape) < 3:\n","            rgb = np.empty((64, 64, 3), dtype=np.float32)\n","            rgb[:, :, 0] = img\n","            rgb[:, :, 1] = img\n","            rgb[:, :, 2] = img\n","            img = rgb\n","\n","        return img.transpose(2, 0, 1)"],"metadata":{"id":"M3eHTs0XBM6R","executionInfo":{"status":"ok","timestamp":1726826901953,"user_tz":-480,"elapsed":823,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# The Generator model\n","class Generator(nn.Module):\n","    def __init__(self, channels, noise_dim=100, embed_dim=1024, embed_out_dim=128):\n","        super(Generator, self).__init__()\n","        self.channels = channels\n","        self.noise_dim = noise_dim\n","        self.embed_dim = embed_dim\n","        self.embed_out_dim = embed_out_dim\n","\n","        # Text embedding layers\n","        self.text_embedding = nn.Sequential(\n","            nn.Linear(self.embed_dim, self.embed_out_dim),\n","            nn.BatchNorm1d(self.embed_out_dim),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","\n","        # Generator architecture\n","        model = []\n","        model += self._create_layer(self.noise_dim + self.embed_out_dim, 512, 4, stride=1, padding=0)\n","        model += self._create_layer(512, 256, 4, stride=2, padding=1)\n","        model += self._create_layer(256, 128, 4, stride=2, padding=1)\n","        model += self._create_layer(128, 64, 4, stride=2, padding=1)\n","        model += self._create_layer(64, self.channels, 4, stride=2, padding=1, output=True)\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, output=False):\n","        layers = [nn.ConvTranspose2d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=False)]\n","        if output:\n","            layers.append(nn.Tanh())  # Tanh activation for the output layer\n","        else:\n","            layers += [nn.BatchNorm2d(size_out), nn.ReLU(True)]  # Batch normalization and ReLU for other layers\n","        return layers\n","\n","    def forward(self, noise, text):\n","        # Apply text embedding to the input text\n","        text = self.text_embedding(text)\n","        text = text.view(text.shape[0], text.shape[1], 1, 1)  # Reshape to match the generator input size\n","        z = torch.cat([text, noise], 1)  # Concatenate text embedding with noise\n","        y = self.model(z)\n","        print(\"generator shapes:\", y.shape)\n","        return y\n","\n","\n","# The Embedding model\n","class Embedding(nn.Module):\n","    def __init__(self, size_in, size_out):\n","        super(Embedding, self).__init__()\n","        self.text_embedding = nn.Sequential(\n","            nn.Linear(size_in, size_out),\n","            nn.BatchNorm1d(size_out),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","\n","    def forward(self, x, text):\n","        embed_out = self.text_embedding(text)\n","        embed_out_resize = embed_out.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)  # Resize to match the discriminator input size\n","        out = torch.cat([x, embed_out_resize], 1)  # Concatenate text embedding with the input feature map\n","        return out\n","\n","\n","# The Discriminator model\n","class Discriminator(nn.Module):\n","    def __init__(self, channels, embed_dim=1024, embed_out_dim=128):\n","        super(Discriminator, self).__init__()\n","        self.channels = channels\n","        self.embed_dim = embed_dim\n","        self.embed_out_dim = embed_out_dim\n","\n","        # Discriminator architecture\n","        self.model = nn.Sequential(\n","            *self._create_layer(self.channels, 64, 4, 2, 1, normalize=False),\n","            *self._create_layer(64, 128, 4, 2, 1),\n","            *self._create_layer(128, 256, 4, 2, 1),\n","            *self._create_layer(256, 512, 4, 2, 1)\n","        )\n","        self.text_embedding = Embedding(self.embed_dim, self.embed_out_dim)  # Text embedding module\n","        self.output = nn.Sequential(\n","            nn.Conv2d(512 + self.embed_out_dim, 1, 4, 1, 0, bias=False), nn.Sigmoid()\n","        )\n","\n","    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, normalize=True):\n","        layers = [nn.Conv2d(size_in, size_out, kernel_size=kernel_size, stride=stride, padding=padding)]\n","        if normalize:\n","            layers.append(nn.BatchNorm2d(size_out))\n","        layers.append(nn.LeakyReLU(0.2, inplace=True))\n","        return layers\n","\n","    def forward(self, x, text):\n","        x_out = self.model(x)  # Extract features from the input using the discriminator architecture\n","        out = self.text_embedding(x_out, text)  # Apply text embedding and concatenate with the input features\n","        out = self.output(out)  # Final discriminator output\n","        out = out.squeeze()\n","        print(\"discriminator shapes: \", out.shape, x_out.shape)\n","        return out, x_out"],"metadata":{"id":"IWupuNxUU5bK","executionInfo":{"status":"ok","timestamp":1726826910016,"user_tz":-480,"elapsed":640,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.utils as vutils\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","import os\n","import time\n","import imageio\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting device to cuda\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using Device\", device)\n","\n","# saving current date and time\n","date = datetime.now().strftime('%Y%m%d')\n","start_time = time.time()\n","\n","\n","# setting up parameters\n","noise_dim = 100\n","embed_dim = 1024\n","embed_out_dim = 128\n","batch_size = 256 #128\n","real_label = 1.\n","fake_label = 0.\n","learning_rate = 0.0002\n","l1_coef = 50\n","l2_coef = 100\n","\n","num_epochs = 1\n","log_interval = 18 #43"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEnDhq1YSTxN","executionInfo":{"status":"ok","timestamp":1726826916249,"user_tz":-480,"elapsed":2245,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}},"outputId":"b519c856-f204-490b-c782-ff05dfa35b49"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Device cuda\n"]}]},{"cell_type":"code","source":["# loading dataset\n","train_dataset = Text2ImageDataset('/content/drive/MyDrive/notebooks/GAN/data/birds.hdf5',split=0) # split { 0: train, 1: validation, 2: test }\n","train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=8)\n","print(\"No of batches: \",len(train_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCELCPG0SoaC","executionInfo":{"status":"ok","timestamp":1726827042069,"user_tz":-480,"elapsed":122204,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}},"outputId":"71d628ab-d351-4e21-ef98-93b754852ddc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["No of batches:  108\n"]}]},{"cell_type":"code","source":["# lists to store losses\n","D_losses = []\n","G_losses = []"],"metadata":{"id":"bwbKx7qPSrYV","executionInfo":{"status":"ok","timestamp":1726827098157,"user_tz":-480,"elapsed":570,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# loss functions\n","criterion = nn.BCELoss()\n","l2_loss = nn.MSELoss()\n","l1_loss = nn.L1Loss()"],"metadata":{"id":"K19NFlpnSp1Q","executionInfo":{"status":"ok","timestamp":1726827100764,"user_tz":-480,"elapsed":3,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# directory to store output images\n","output_save_path = '/content/drive/MyDrive/notebooks/GAN/data/generated_images/'\n","os.makedirs(output_save_path, exist_ok=True)"],"metadata":{"id":"G0tiWVHnXaBZ","executionInfo":{"status":"ok","timestamp":1726827101844,"user_tz":-480,"elapsed":2,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# initializing generator\n","generator = Generator(channels=3, embed_dim=embed_dim, noise_dim=noise_dim, embed_out_dim=embed_out_dim).to(device)\n","generator.apply(weights_init)\n","\n","\n","# initializing discriminator\n","discriminator = Discriminator(channels=3, embed_dim=embed_dim, embed_out_dim=embed_out_dim).to(device)\n","discriminator.apply(weights_init)\n","\n","# setting up Adam optimizer for Generator and Discriminator\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))"],"metadata":{"id":"D-c-dGuwStuQ","executionInfo":{"status":"ok","timestamp":1726827103329,"user_tz":-480,"elapsed":709,"user":{"displayName":"Cuburt Rivera Balanon","userId":"13452976318629155965"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# training loop\n","\n","# iterating over number of epochs\n","for epoch in range(num_epochs):\n","\n","    batch_time = time.time()\n","\n","    #iterating over each batch\n","    for batch_idx,batch in enumerate(train_loader):\n","\n","        # reading the data into variables and moving them to device\n","        images = batch['right_images'].to(device)\n","        wrong_images = batch['wrong_images'].to(device)\n","        embeddings = batch['right_embed'].to(device)\n","        batch_size = images.size(0)\n","\n","        # ================================================================== #\n","        #                      Train the discriminator                       #\n","        # ================================================================== #\n","\n","        # Clear gradients for the discriminator\n","        optimizer_D.zero_grad()\n","\n","        # Generate random noise\n","        noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)\n","\n","        # Generate fake image batch with the generator\n","        fake_images = generator(noise, embeddings)\n","\n","        # Forward pass real batch and calculate loss\n","        real_out, real_act = discriminator(images, embeddings)\n","        d_loss_real = criterion(real_out, torch.full_like(real_out, real_label, device=device))\n","\n","        # Forward pass wrong batch and calculate loss\n","        wrong_out, wrong_act = discriminator(wrong_images, embeddings)\n","        d_loss_wrong = criterion(wrong_out, torch.full_like(wrong_out, fake_label, device=device))\n","\n","        # Forward pass fake batch and calculate loss\n","        fake_out, fake_act = discriminator(fake_images.detach(), embeddings)\n","        d_loss_fake = criterion(fake_out, torch.full_like(fake_out, fake_label, device=device))\n","\n","        # Compute total discriminator loss\n","        d_loss = d_loss_real + d_loss_wrong + d_loss_fake\n","\n","        # Backpropagate the gradients\n","        d_loss.backward()\n","\n","        # Update the discriminator\n","        optimizer_D.step()\n","\n","        # ================================================================== #\n","        #                        Train the generator                         #\n","        # ================================================================== #\n","\n","        # Clear gradients for the generator\n","        optimizer_G.zero_grad()\n","\n","        # Generate new random noise\n","        noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)\n","\n","        # Generate new fake images using Generator\n","        fake_images = generator(noise, embeddings)\n","\n","        # Get discriminator output for the new fake images\n","        out_fake, act_fake = discriminator(fake_images, embeddings)\n","\n","        # Get discriminator output for the real images\n","        out_real, act_real = discriminator(images, embeddings)\n","\n","        # Calculate losses\n","        g_bce = criterion(out_fake, torch.full_like(out_fake, real_label, device=device))\n","        g_l1 = l1_coef * l1_loss(fake_images, images)\n","        g_l2 = l2_coef * l2_loss(torch.mean(act_fake, 0), torch.mean(act_real, 0).detach())\n","\n","        # Compute total generator loss\n","        g_loss = g_bce + g_l1 + g_l2\n","\n","        # Backpropagate the gradients\n","        g_loss.backward()\n","\n","        # Update the generator\n","        optimizer_G.step()\n","\n","        # adding loss to the list\n","        D_losses.append(d_loss.item())\n","        G_losses.append(g_loss.item())\n","\n","        # progress based on log_interval\n","        if (batch_idx+1) % log_interval == 0 and batch_idx > 0:\n","            print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n","                          epoch+1, batch_idx+1, len(train_loader),\n","                          d_loss.mean().item(),\n","                          g_loss.mean().item(),\n","                          time.time() - batch_time))\n","\n","        # storing generator output after every 10 epochs\n","        if batch_idx == len(train_loader)-1 and ((epoch+1)%10==0 or epoch==0):\n","            viz_sample = torch.cat((images[:32], fake_images[:32]), 0)\n","            vutils.save_image(viz_sample,\n","            os.path.join(output_save_path, 'output_{}_epoch_{}.png'.format(date,epoch+1)),\n","                              nrow=8,normalize=True)\n","\n","# saving the trained models\n","torch.save(generator.state_dict(), os.path.join(model_save_path, 'generator_{}.pth'.format(date)))\n","torch.save(discriminator.state_dict(), os.path.join(model_save_path,'discriminator_{}.pth'.format(date)))\n","\n","print('Total train time: {:.2f}'.format(time.time() - start_time))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dUiaO_2SwKI","outputId":"a373a6b4-8327-43bf-8c47-f61961b11d73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","generator shapes: torch.Size([256, 3, 64, 64])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n","discriminator shapes:  torch.Size([256]) torch.Size([256, 512, 4, 4])\n"]}]},{"cell_type":"code","source":["loaded_model = model\n","loaded_model.eval()\n","loaded_model.freeze()\n","loaded_model = loaded_model.to(DEFAULT_DEVICE)"],"metadata":{"id":"-61TMbSa7bu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example text descriptions\n","text_descriptions = [\"a number five with thick lines\"]\n","\n","# Encode the text descriptions using CLIP\n","text_features = loaded_model.encode_text(text_descriptions)"],"metadata":{"id":"iNHZHSAr7c4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_images(model, text_features, num_samples=1):\n","    model.eval()\n","    z = torch.randn(num_samples, model.noise_dim).to(model.device)  # Generate random noise\n","    with torch.no_grad():\n","        generated_images = model(z, text_features)\n","    return generated_images"],"metadata":{"id":"GzaygtrZ7fa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_samples = len(text_descriptions)\n","generated_images = generate_images(loaded_model, text_features, num_samples)"],"metadata":{"id":"PoqCuS-z7gyO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms import ToPILImage\n","\n","\n","def to_pil_images(tensors):\n","    images = (tensors + 1) / 2  # Normalize from [-1, 1] to [0, 1]\n","    images = [ToPILImage()(img.view(32, 32).cpu()) for img in images]\n","    return images\n","\n","pil_images = to_pil_images(generated_images)\n","\n","# Display the generated images\n","for img in pil_images:\n","    plt.imshow(img)"],"metadata":{"id":"MA0JTbjx7jUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AFeFBmXcEqyV"},"execution_count":null,"outputs":[]}]}