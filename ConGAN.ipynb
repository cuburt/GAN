{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2354e-4151-4c62-873e-635f6d5c461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning transformers torch torchvision matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164b93b4-6cea-4116-b55b-c172c68db752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuburtbalanon/anaconda3/envs/gan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104aef930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "%matplotlib inline\n",
    "torch.set_num_threads(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b8d9ac-7114-4451-98b2-9f96b10d13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Version: 3.10.14\n",
      "torch Version: 2.3.0\n",
      "torchvision Version: 0.18.0\n",
      "GPU: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"python Version: {sys.version.split(' ')[0]}\")\n",
    "print(f\"torch Version: {torch.__version__}\")\n",
    "print(f\"torchvision Version: {torchvision.__version__}\")\n",
    "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"GPU: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb983ac7-4620-4645-aee6-1cff024521b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, condition_dim, image_channels, generator_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator_size = generator_size\n",
    "        self.noise_encoder = nn.Sequential(nn.Linear(noise_dim, generator_size // 2))\n",
    "        self.condition_encoder = nn.Sequential(nn.Linear(condition_dim, generator_size // 2))\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(generator_size, generator_size, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(generator_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(generator_size, generator_size // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(generator_size // 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(generator_size // 2, generator_size // 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(generator_size // 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(generator_size // 8, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, condition):\n",
    "        noise_embedding = self.noise_encoder(noise)\n",
    "        condition_embedding = self.condition_encoder(condition)\n",
    "        z = torch.cat([noise_embedding, condition_embedding], dim=1).reshape(-1, self.generator_size, 1, 1)\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d32ed67-b921-4927-87ca-dcec1cbfaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, condition_dim, discriminator_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.condition_encoder = nn.Sequential(nn.Linear(condition_dim, discriminator_size))\n",
    "        self.noise_encoder = nn.Sequential(nn.Conv2d(1, discriminator_size // 4, 3, 2),\n",
    "                                          nn.InstanceNorm2d(discriminator_size // 4, affine=True),\n",
    "                                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                                          nn.Conv2d(discriminator_size // 4, discriminator_size // 2, 3, 2),\n",
    "                                          nn.InstanceNorm2d(discriminator_size // 2, affine=True),\n",
    "                                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                                          nn.Conv2d(discriminator_size // 2, discriminator_size, 3, 2),\n",
    "                                          nn.InstanceNorm2d(discriminator_size, affine=True),\n",
    "                                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                                          nn.Flatten())\n",
    "        self.model = nn.Sequential(nn.Linear(discriminator_size * 8, discriminator_size),\n",
    "                                  nn.LeakyReLU(0.2, inplace=True),\n",
    "                                  nn.Linear(discriminator_size, 1))\n",
    "\n",
    "    def forward(self, noise, condition):\n",
    "        condition_embedding = self.condition_encoder(condition)\n",
    "        noise_embedding = self.noise_encoder(noise)\n",
    "        z = torch.cat([noise_embedding, condition_embedding], dim=1)\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30fe54c-0aa7-4fa6-8dc2-a7b0bd1600c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self, noise_dim=100, condition_dim=10, image_channels=1, generator_size=512, discriminator=512, lr=0.0001, b1=0.5, b2=0.999):\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization=False\n",
    "\n",
    "        self.generator = Generator(noise_dim, condition_dim, image_channels, generator_size)\n",
    "        self.discriminator = Discriminator(condition_dim, discriminator_size)\n",
    "\n",
    "    def forward(self, noise, condition):\n",
    "        return self.generator(noise, condition)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = optim.AdamW(self.generator.parameters(), lr=self.hparams.lr, betas=(self.hparams.b1, self.hparams.b2))\n",
    "        opt_d = optim.AdamW(self.discriminator.parameters(), lr=self.hparams.lr, betas=(self.hparams.b1, self.hparams.b2))\n",
    "        return [opt_g, opt_d], []\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        real_images, real_class_labels = batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1124d66c-af17-4733-a8f5-3e479e1ad9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotatedMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super(AnnotatedMNISTDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass  # No data download needed\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize((0.5,), (0.5,))\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        self.train_dataset = Annotated_MNIST(train=True)\n",
    "        self.val_dataset = Annotated_MNIST(train=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, text_descriptions = zip(*batch)\n",
    "        images = torch.stack([self.transform(img) for img in images])\n",
    "        # print(images, list(text_descriptions))\n",
    "        return images, list(text_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da721b47-5bf2-48f1-ad94-2d037e0e2304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
